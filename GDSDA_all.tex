% This is LLNCS.DOC the documentation file of
% the LaTeX2e class from Springer-Verlag
% for Lecture Notes in Computer Science, version 2.4
\documentclass{llncs}

\usepackage{llncsdoc}
\let \proof \relax
\let\endproof\relax
\usepackage{amsthm,amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{subfig}
%\usepackage{subfigure}
\usepackage{lipsum}
\usepackage{array}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{color,soul}
\usepackage{epstopdf}
%\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
%\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
%\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
%\newcommand{\algorithmicbreak}{\textbf{break}}
%\newtheorem{theorem}{Theorem}
%\newtheorem{corollary}{Corollary}
%\newtheorem{lemma}{Lemma}
%
\begin{document}

\title{Fast Generalized Distillation for Semi-supervised Domain Adaptation}
\maketitle
\begin{abstract}
	Semi-supervised domain adaptation (SDA) is a typical setting when we face the problem of domain adaptation in real applications. How to effectively utilize the unlabeled data is an important issue in SDA. In this paper, we propose a new paradigm, called \textit{Generalized Distillation Semi-supervised Domain Adaptation} (GDSDA) to solve the SDA problem.
	We first demonstrate that GDSDA can effectively utilize the unlabeled data to transfer the knowledge from the source models. We illustrate that the imitation parameter of GDSDA can greatly affect the performance of the target model and propose GDSDA-SVM which uses SVMs as the base classifier and can effectively estimate the imitation parameter. Specifically, the imitation parameter is estimated by minimizing the Leave-one-out cross-validation loss on the target data using our novel objective function. Experiment results show that GDSDA-SVM can effectively utilize the unlabeled data to transfer the knowledge between different domains under the SDA setting.
\end{abstract}

\section{Introduction}
\input{intro2}


%\section{Related Work}\label{sec:work}
%\input{work2}

\section{Generalized Distillation for Semi-supervised Domain Adaptation}\label{sec:gdda}
\input{gd}
\input{GDDA}

\section{GDSDA-SVM}\label{sec:svm}
\input{multi-distill.tex}

\section{Experiments}\label{sec:exp}
\input{exp}

\section{Conclusion}\label{sec:con}
In this paper, we propose a framework called {Generalized Distillation Semi-supervised Domain Adaptation} that can effectively leverage the knowledge from the source domain using the unlabeled data of the SDA problem. To make GDSDA more effective in real applications, we proposed a method called GDSDA-SVM and show that GDSDA-SVM can effectively estimate the imitation parameter for GDSDA. Experiment results show that GDSDA-SVM can effectively leverage the knowledge from one or more source models for the real SDA applications.



\bibliographystyle{splncs03}
\bibliography{research}


%\bibauthoryear

\end{document}
