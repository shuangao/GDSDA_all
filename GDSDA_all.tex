% This is LLNCS.DOC the documentation file of
% the LaTeX2e class from Springer-Verlag
% for Lecture Notes in Computer Science, version 2.4
\documentclass{llncs}

\usepackage{llncsdoc}
\let \proof \relax
\let\endproof\relax
\usepackage{amsthm,amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{subfig}
%\usepackage{subfigure}
\usepackage{lipsum}
\usepackage{array}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{color,soul}
\usepackage{epstopdf}
%\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
%\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
%\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
%\newcommand{\algorithmicbreak}{\textbf{break}}
%\newtheorem{theorem}{Theorem}
%\newtheorem{corollary}{Corollary}
%\newtheorem{lemma}{Lemma}
%
\begin{document}

\title{Fast Generalized Distillation for Semi-supervised Domain Adaptation}
\maketitle
\begin{abstract}
	Semi-supervised domain adaptation (SDA) is a typical setting when we face the problem of domain adaptation in real applications. How to effectively utilize the unlabeled data is an important issue in SDA.
	Previous work requires to access the source data to measure the data distribution mismatch which is ineffective when the size of the source data is relatively large.
	In this paper, we propose a new paradigm, called \textit{Generalized Distillation Semi-supervised Domain Adaptation} (GDSDA) that can effectively utilize the unlabeled data to transfer the knowledge from the source models to solve the SDA problem. We demonstrate that the value of the imitation parameter is crucial to the performance of the target model in GDSDA. Therefore, we propose GDSDA-SVM which uses SVM as the base classifier and can efficiently estimate the imitation parameter.  
	Experiment results show that GDSDA-SVM can effectively utilize the unlabeled data to transfer the knowledge between different domains under the SDA setting.
\end{abstract}

\section{Introduction}
\input{intro2}


%\section{Related Work}\label{sec:work}
%\input{work2}

\section{Generalized Distillation for Semi-supervised Domain Adaptation}\label{sec:gdda}
\input{gd}
\input{GDDA}

\section{GDSDA-SVM}\label{sec:svm}
\input{multi-distill.tex}

\section{Experiments}\label{sec:exp}
\input{exp}

\section{Conclusion}\label{sec:con}
In this paper, we propose a framework called {Generalized Distillation Semi-supervised Domain Adaptation} that can effectively leverage the knowledge from the source model using the unlabeled data to solve the SDA problem. To make GDSDA more efficient in real applications, we proposed a method called GDSDA-SVM and show that GDSDA-SVM can effectively estimate the imitation parameter for GDSDA. Experiment results show that GDSDA-SVM can effectively leverage the knowledge from one or more source models in real SDA applications.



\bibliographystyle{splncs03}
\bibliography{research}


%\bibauthoryear

\end{document}
