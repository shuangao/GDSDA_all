\relax 
\citation{duan2009domain}
\citation{daume2010frustratingly}
\citation{yao2015semi}
\citation{Donahue_2013_CVPR}
\@writefile{toc}{\contentsline {title}{Fast Generalized Distillation for Semi-supervised Domain Adaptation}{1}}
\@writefile{toc}{\authcount {1}}
\@writefile{toc}{\contentsline {author}{No Author Given}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{lopez2015unifying}
\citation{lopez2015unifying}
\citation{lopez2015unifying,Tzeng_2015_ICCV}
\citation{hinton2015distilling}
\citation{vapnik2015learning}
\@writefile{toc}{\contentsline {section}{\numberline {2}Generalized Distillation for Semi-supervised Domain Adaptation}{3}}
\newlabel{sec:gdda}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}An overview of Generalized Distillation and GDSDA}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:gd}{{\caption@xref {fig:gd}{ on input line 4}}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of Generalized Distillation training process.\relax }}{3}}
\newlabel{eq:softmax_T}{{1}{3}}
\newlabel{eq:distill}{{2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Illustration of GDSDA training process and the ``fake label" strategy.\relax }}{4}}
\newlabel{fig:GDSDA}{{2}{4}}
\citation{Donahue_2013_CVPR,duan2012visual}
\citation{daume2010frustratingly}
\citation{duan2012learning}
\citation{hinton2015distilling}
\citation{karl2001long}
\newlabel{eq:GDDA_abs}{{3}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Why does GDSDA work?}{5}}
\citation{duan2012learning,duan2012visual}
\citation{pan2010survey}
\citation{lopez2015unifying}
\citation{Tzeng_2015_ICCV}
\citation{cawley2006leave}
\citation{duan2009domain}
\citation{ba2014deep,luo2016face,romero2014fitnets,urban2016deep}
\@writefile{toc}{\contentsline {section}{\numberline {3}GDSDA-SVM}{6}}
\newlabel{sec:svm}{{3}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Distillation with multiple sources}{6}}
\newlabel{eq:multi-distill}{{4}{6}}
\citation{cawley2006leave}
\newlabel{eq:yhat}{{8}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Cross-entropy loss for imitation parameter estimation}{7}}
\newlabel{eq:loo_loss}{{9}{7}}
\newlabel{eq:ce}{{10}{7}}
\citation{KrizhevskyNIPS12}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces GDSDA-SVM\relax }}{8}}
\newlabel{alg:svm}{{1}{8}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces $\lambda $ Optimization\relax }}{8}}
\newlabel{alg:lambda}{{2}{8}}
\newlabel{eq:p}{{11}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{8}}
\newlabel{sec:exp}{{4}{8}}
\citation{fan2008liblinear}
\citation{delalleau2005efficient}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Single Source for Office datasets}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Experiment results on DSLR$\rightarrow $Amazon and Webcam$\rightarrow $Amazon when there are just one labeled examples per class. The results of DSLR$\rightarrow $Amazon and Webcam$\rightarrow $Amazon are shown in figure (a)-(c) and (d)-(e) respectively. GDSDA-SVM is trained with temperature $T=20$. The X-axis denotes the imitation parameter of the hard label (i.e. $\lambda _1$ in Fig 2\hbox {}) and the corresponding imitation parameter of the soft label is set to $1-\lambda _1$. \relax }}{10}}
\newlabel{fig:single1}{{3}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {D $\rightarrow $ A, 10 unlabeled }}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {D $\rightarrow $ A, 15 unlabeled }}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {D $\rightarrow $ A, 20 unlabeled }}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {W $\rightarrow $ A, 10 unlabeled }}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {W $\rightarrow $ A, 15 unlabeled }}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {W $\rightarrow $ A, 20 unlabeled }}}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Multi-Source for Office datasets}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces D+W$\rightarrow $A, Multi-source results comparison.\relax }}{10}}
\newlabel{fig:multi}{{4}{10}}
\bibstyle{splncs03}
\bibdata{research}
\bibcite{ba2014deep}{1}
\bibcite{cawley2006leave}{2}
\bibcite{daume2010frustratingly}{3}
\bibcite{delalleau2005efficient}{4}
\bibcite{Donahue_2013_CVPR}{5}
\bibcite{duan2009domain}{6}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{11}}
\newlabel{sec:con}{{5}{11}}
\bibcite{duan2012learning}{7}
\bibcite{duan2012visual}{8}
\bibcite{fan2008liblinear}{9}
\bibcite{hinton2015distilling}{10}
\bibcite{karl2001long}{11}
\bibcite{KrizhevskyNIPS12}{12}
\bibcite{lopez2015unifying}{13}
\bibcite{luo2016face}{14}
\bibcite{pan2010survey}{15}
\bibcite{romero2014fitnets}{16}
\bibcite{Tzeng_2015_ICCV}{17}
\bibcite{urban2016deep}{18}
\bibcite{vapnik2015learning}{19}
\bibcite{yao2015semi}{20}
