Domain adaptation can be used in many real applications, which addresses the problem of learning a target domain with the help of a different but related source domain. 
In real applications, it can be very expensive to obtain sufficient labeled examples while there are abundant unlabeled ones. 
\textit{Semi-supervised domain adaptation} (SDA) tries to exploit the knowledge from the source domain and use a certain amount of unlabeled examples and a few labeled ones from the target domain to learn a target model. Typically, the labeled examples in the target domain are too few to construct a good classifier alone. How to effectively utilize the unlabeled examples is an important issue in SDA. 

In previous work of SDA, many methods have been proposed to leverage the source knowledge with the unlabeled data.
Duan et al.\cite{duan2012visual} proposed a method to measure the domain shift with Maximum Mean Discrepancy (MMD) between source and target domains using the labeled and unlabeled data. Daum{\'e} et al\cite{daume2010frustratingly} utilized unlabeled data as a co-regularizer and forced the hypotheses learned from different domains to agree on the unlabeled data. Meanwhile, Yao et al.\cite{yao2015semi} used the unlabeled target examples to discover the underlying intrinsic information in the target domain. Donahue et al.\cite{Donahue_2013_CVPR} show that using the smoothness constraints on the classifier scores over the unlabeled data can lead to the improved transfer result.
The previous work in SDA requires to access the source data to measure the data distribution mismatch between the source and target domain.
However, in some situations, we may not be able to access each of the source examples for many reasons. When we use a large dataset as our source domain, for example, it is tedious to compare each of the source examples with the target data to estimate the data distribution mismatch.

Recently, a framework called \textit{Generalized Distillation} (\textbf{GD})\cite{lopez2015unifying} was proposed, which allows the knowledge to be transferred between different models effectively. GD includes two different models, the teacher model and student model. The student model tries to learn from the teacher model by mimicking the outputs of the teacher model on the training data. Remarkably, in GD, the knowledge can be directly transferred from the teacher model to the student model without accessing the data used to train the teacher. Moreover, GD can be used to exploit the information of the unlabeled data in a semi-supervised scenario\cite{lopez2015unifying}.
Given that GD has such ability, it is natural to ask the following two questions: (1) Can GD framework be applied to solve the SDA problem? (2) How to improve its effectiveness when we apply GD to real SDA applications?

To answer these two questions, in this paper, we first propose a new paradigm, called \textit{Generalized Distillation Semi-supervised Domain Adaptation} (\textbf{GDSDA}), to solve the SDA problem. 
We show that, with GDSDA the knowledge of the source models can be effectively transferred to the target domain using the unlabeled data. Specifically, the target model is trained with the help of the soft labels, i.e. the predictions of the target domain examples given by the source models. Therefore, without accessing each of the source examples, GDSDA is more efficient especially when the source domain is relatively large and there is a well-trained source model.

Then we argue that the imitation parameter of GDSDA which controls the amount of knowledge transferred from the source model can greatly affect the performance of the target model.
However, according to the previous work\mbox{\cite{lopez2015unifying,Tzeng_2015_ICCV}}, the imitation parameter is a hyperparameter and can only be determined by either brute force search or background knowledge. 
Therefore, we propose a novel imitation parameter estimation method for GDSDA, called GDSDA-SVM, which uses SVM as the base classifier and can determine the imitation parameter automatically. In particular, we use the Mean Square Error loss for GDSDA-SVM and show that the Leave-one-out cross validation (LOOCV) loss can be calculated in a closed form. By minimizing the LOOCV loss on the target training data, we can find the optimal imitation parameter for the target model. In our experiments, we show that GDSDA-SVM can effectively find the optimal imitation parameter and achieve competitive performance compared to methods using brutal force search but with faster speed. 

To summarize, the main contributions of this paper include: (1) We propose the paradigm of GDSDA that can directly transfer the knowledge from the source model with the help of unlabeled data for the SDA problems. (2) We propose the GDSDA-SVM that can effectively find the optimal imitation parameter for real SDA applications.

%The rest of this paper is organized as follow: In Section \ref{sec:work}, we describe the related work on privileged information and distillation in domain adaptation. Section \ref{sec:gdda} we propose our framework of GDSDA and provide some statistic analysis. Based on that, we propose our GDSDA-SVM in Section \ref{sec:svm}. Experimental results are shown in Section \ref{sec:exp}. Some discussion and conclusion are provided in Section \ref{sec:con}.