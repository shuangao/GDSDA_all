Domain adaptation can be used in many real applications, which addresses the problem of learning a target domain with the help of a different but related source domain. 
%Previous methods show that carefully modeling the source data to compensate for the domain shift between different domains can significantly improve the performance on the target domain \cite{Donahue_2013_CVPR}. 
In real applications, it can be very expensive to obtain sufficient labeled examples while there are abundant unlabeled examples. 
\textit{Semi-supervised domain adaptation} (SDA) tries to utilize the knowledge from the source domain and use some unlabeled examples and  a few labeled ones from the target domain to compensate for the domain shift\cite{karl2001long}. Typically, the labeled examples in the target domain are too few to construct a good classifier alone. Therefore, how to effectively utilize the unlabeled examples is an important issue in SDA. 

In previous work of SDA, many methods have been proposed to leverage the source knowledge with the unlabeled data.
Duan et al.\cite{duan2012visual} proposed a method to measure the domain shift with Maximum Mean Discrepancy of the labeled and unlabeled data from source and target domains. Daum{\'e} et al\cite{daume2010frustratingly} utilized unlabeled data as a co-regularizer and forced the hypotheses learned from different domains to agree on the unlabeled data. Yao et al.\cite{yao2015semi} used the unlabeled target examples to discover the underlying intrinsic information in the target domain. Donahue et al.\cite{Donahue_2013_CVPR} show that using smoothness constraints on the classifier scores over the unlabeled data can lead to the improved transfer result.
Most previous work in SDA requires to access the individual example in the source domain to measure the data distribution mismatch between the source and target domain.
However, in some situation, we may not be able to access each of the source examples for many reasons. For example, when we use a large dataset as our source domain, it is tedious to access each of the source examples to estimate the data distribution mismatch.

Recently, a framework called \textit{Generalized Distillation} (\textbf{GD})\cite{lopez2015unifying} was proposed, which allows the knowledge to be transferred between different models effectively. GD includes two different models, the teacher and student models. The student model tries to distill the knowledge from the teacher model by mimicking the outputs of the teacher model on the training data. Remarkably, in GD, the knowledge can be directly transferred from the teacher model to the student learner without utilizing any of the data used to train the teacher.
Given that GD has such ability, it is natural to ask the following two questions: (1) Can GD be applied to solve the SDA problem? (2) Is there any obstacle when we apply GD to real SDA applications?

To answer these two questions, in this paper, we first propose a new paradigm, called \textit{Generalized Distillation Semi-supervised Domain Adaptation} (\textbf{GDSDA}), to solve the SDA problem. 
We show that the knowledge of the source models can be effectively transferred to the target domain in GDSDA using the unlabeled data. Specifically, the target model is trained with the help of the soft labels, i.e. the predictions of the target domain examples from the source model. Therefore, without accessing each of the individual examples in the source domain, GDSDA is more efficient especially when the source domain is relatively large and there is a well-trained source model.

Then we argue that the imitation parameter of GDSDA which controls the amount of knowledge transferred from the source can greatly affect the performance of the target model.
However, according to previous work\cite{lopez2015unifying,Tzeng_2015_ICCV}, the imitation parameter is a hyperparameter and can only be determined by either brute force search or background knowledge. 
Therefore, we propose a novel imitation parameter estimation method for GDSDA, called GDSDA-SVM that uses SVM as the base classifier and can determine the imitation parameter automatically. In particular, we use mean square loss for GDSDA-SVM and show that the Leave-one-out cross validation (LOOCV) loss can be calculated in a closed form. By minimizing the LOOCV loss on the target training data, we can find the optimal imitation parameter for the target model. In our experiments, we show that GDSDA-SVM can effectively find the optimal imitation parameter and achieve competitive performance compared to methods using brutal force search but with faster speed. 

To summarize, the main contributions of this paper include: (1) We propose the framework GDSDA for domain adaptation and show that GDSDA can be used in many real SDA problems. (2) We propose the GDSDA-SVM that can effectively find the optimal imitation parameter for GDSDA.

%The rest of this paper is organized as follow: In Section \ref{sec:work}, we describe the related work on privileged information and distillation in domain adaptation. Section \ref{sec:gdda} we propose our framework of GDSDA and provide some statistic analysis. Based on that, we propose our GDSDA-SVM in Section \ref{sec:svm}. Experimental results are shown in Section \ref{sec:exp}. Some discussion and conclusion are provided in Section \ref{sec:con}.